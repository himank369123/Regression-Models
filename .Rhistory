library(collegeIncome)
data(college)
devtools::install_github("jhudsl/matahari")
library(collegeIncome)
data(college)
dance_start(value = FALSE, contents = FALSE)
library(matahari)
dance_start(value = FALSE, contents = FALSE)
str(college)
fit<-lm(median~major_category,data=college)
hist(college$median)
any(is.na(college$major_category))
factor(college$major_category)
df<-aggregate(college$median~college$major_category,FUN = mean)
names(df)<-c("major","medsal")
par(mfrow=c(1,1))
barplot(df$medsal)
summary(fit)$coef
summary(fit)$coef[,4]<0.05
rpois(mean=3)
rpois(20,mean=3)
rpois(20)
?rpois
rpois(20,lambda = 3)
var(rpois(20,lambda = 3))
var(rpois(1000,lambda = 3))
var(rpois(1000,lambda = 3))
library(car)
fit<-lm(data=swiss,Fertility~.)
vif(fit)
fit<-lm(data=swiss,Fertility~.)
vif(fit)
## this means.by the inclusion of agriculture the variance inflates by 2.28 times than it would if agriculture was uncorelated.
##note vif for infant.mortality is only 1 since its mostly uncorelated with other variables.
##therefore always include variables with less variance inflation factor (Vif)
## we can conduct nested variable models and then anova over them as below:
fit1<-lm(data=swiss,Fertility~Agriculture)
fit2<-lm(data=swiss,Fertility~Agriculture+Education+Examination)
fit3<-lm(data=swiss,Fertility~.)
anova(fit1,fit2,fit3)
library(datasets)
data(swiss)
names(swiss)
fit<-lm(data=swiss,Fertility~.)
summary(fit)
#1% increase in agriculture is associated with 0.17% decrease in fertility keeping others constant.
#t vlaue tests Ho hypothesis if agriculture coefficient is 0. ie 0.17211-0/S.E
#p value give significance. in this case significant at 5% level. reject NUll hypothesis.
fit1<-lm(data=swiss,Fertility~Agriculture)
fit1
#this suggests 1% increase in agriculture is associated with 0.19 % increase in fertility.
#this is opposite of what observed before. this is simpson's paradox.
#therefore multiple regression is accurate when all necessary variables are included.
### FACTORE IN MULTIPLE REGRESSION:
#FACTORS GIVE RELATIVE COEFFICIENTS. THE INTERCEPT IS COEFF FOR ONE VARIABLE. AND OTHER COEFFS ARE RELATIVE TO IT.
data("InsectSprays")
library(ggplot2)
g<-ggplot(data=InsectSprays,aes(y=count,x=spray,fill=spray))
g<-g+geom_violin(colour="black",size=2)
g
fit<-lm(data=InsectSprays,count~spray)
summary(fit)
#14.5 is mean for spray A.
#0.833 is the diffrenece between spray B and spray A
#SPRAY X = SPRAY A + COEFF
#p value tests wheather a spray mean is equal to mean of SPRAY A
#remove intercept by -1 in lm
#here p value shows if a spray mean is equal to 0
fit2<-lm(data=InsectSprays,count~spray-1)
summary(fit2)
#adjustment in multiple regression means accounting for cofounding variable(s)
# we can change reference level by using relevel()
################################################################################
# y=b+mx1
data(swiss)
swiss
library(dplyr)
swiss<-mutate(swiss,Catholicbin=1*(Catholic>50))
swiss$Catholicbin<-factor(swiss$Catholicbin)
fit<-lm(data=swiss,Fertility~Agriculture)
intercept<-coef(fit)[1]
slope<-coef(fit)[2]
g<-ggplot(data=swiss,aes(x=Agriculture,y=Fertility,color=Catholicbin))
g<-g+geom_point(size=5,colour="black")+geom_point(size=3)
g<-g+geom_abline(intercept = intercept,slope=slope,size=2)
g
#####################
# y=b+m1x1+m2x2
fit<-lm(data=swiss,Fertility~Agriculture+Catholicbin)
summary(fit)
coef(fit)
#fertility increases by 0.124 for every 1 increase in agriculture.
#p values suggest that agriculture isnt statistically significant, catholicbin is statistically significant.
#here the slope is same. catholicbin1 represents change in intercept for catholicbin1. i.e change in fertility from protestent to catholic
g<-ggplot(data=swiss,aes(x=Agriculture,y=Fertility,color=Catholicbin))
g<-g+geom_point(size=5,color='black')+geom_point(size=3)
g<-g+geom_abline(intercept = coef(fit)[1],slope=coef(fit)[2],color='black',size=2)+geom_abline(intercept = coef(fit)[1],slope=coef(fit)[2],color='salmon',size=1)
g<-g+geom_abline(intercept = coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2],color='black',size=2)+geom_abline(intercept = coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2],color='cyan',size=1)
g
############################
# if we want different slopes different intercepts we need  an asteric (*) in model lm.
# this shows interaction between x1 & x2. if slopes are different then there is interaction.
#interaction would mean that agriculture has different affect on fertility depending on wheather they are catholic or protestent.
# y=b+m1x1+m2x2+m3x1x2
fit<-lm(data=swiss,Fertility~Agriculture*factor(Catholicbin))
summary(fit)
coef(fit)
#fertility changes by 0.096 for every unit increase in agriculture for catholicbin0
#fertility changes by 0.096+0.089 for every unit increase in agriculture for catholicbin1
#p values suggest that none of the predictors are statistically significant and even the interaction isnt.
g<-g<-ggplot(data=swiss,aes(x=Agriculture,y=Fertility,color=Catholicbin))
g<-g+geom_point(size=5,color='black')+geom_point(size=3)
g<-g+geom_abline(intercept = coef(fit)[1],slope=coef(fit)[2],color='black',size=2)+geom_abline(intercept = coef(fit)[1],slope=coef(fit)[2],color='salmon',size=1)
g<-g+geom_abline(intercept = coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2]+coef(fit)[4],color='black',size=2)+geom_abline(intercept = coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2]+coef(fit)[4],color='cyan',size=1)
g
#note: interaction simply means the effect of a third variable on outcome for a predictor.
#meanig, does the outcome change for same value of predictor but different interaction variable value.
data("InsectSprays")
library(ggplot2)
g<-ggplot(data=InsectSprays,aes(y=count,x=spray,fill=spray))
g<-g+geom_violin(colour="black",size=2)
g
fit<
fit<-lm(data=InsectSprays,count~spray)
summary(fit)
fit<-lm(data=InsectSprays,count~spray)
summary(fit
summary(fit)
summary(fit)
library(ggplot2)
library(UsingR)
data(diamond)
fit<-lm(data=diamond,price~carat)
rm(g)
g<-ggplot(data=diamond,aes(x=carat,y=price))
g<-g+geom_point(aes(diamond$carat,diamond$price))
g<-g+geom_smooth(method="lm")
g
plot(x=diamond$carat,y=diamond$price,pch=16)
abline(fit,lwd=2)
yhat<-predict(fit)
x=diamond$carat;y=diamond$price
n<-length(y)
for(i in 1:n){
lines(c(x[i],x[i]),c(y[i],yhat[i]),col="red",lwd=2 )
}
e<-resid(fit)
plot(x,e)
abline(h=0)
for (i in 1:n){
lines(c(x[i],x[i]),c(0,e[i]),col="red")
}
##not correct model. but does explain a linear trend. no model is useless.
x<-runif(100,-3,3)
y<-x+sin(x)+rnorm(100,0.1,0.1)
fit<-lm(y~x)
rm(g)
g<-ggplot(data=data.frame(x=x,y=y),aes(x,y))
g<-g+geom_point(size=3,col="black",alpha=0.2)
g<-g+geom_point(size=2,col="red",alpha=0.3)
g<-g+geom_smooth(method="lm")
g
#now we can use residual to highlight a trend that is not explained by linearity by residual plot:
e<-resid(fit)
plot(x,e)
abline(h=0)
#this explains the sin trend using residual variation from linear model.
#Heteroscedasticity is unequal variance which can be emphasized using residual plots.
#basically linear model predicts good at low x values but more variation at big x values.
x<-runif(100,0,0.6)
y<-x+rnorm(100,mean=0,sd=.001*x)
plot(x,y)
fit<-lm(y~x)
abline(fit)
e<-resid(fit)
plot(x,e)
abline(h=0)
g<-g+geom_point(aes(diamond$carat,diamond$price))
g<-g+geom_smooth(method="lm")
g
g<-ggplot(data=diamond,aes(x=carat,y=price))
g<-g+geom_point(aes(diamond$carat,diamond$price))
g<-g+geom_smooth(method="lm")
g
plot(x=diamond$carat,y=diamond$price,pch=16)
abline(fit,lwd=2)
abline(fit,lwd=2)
fit<-lm(data=diamond,price~carat)
abline(fit,lwd=2)
yhat<-predict(fit)
yhat<-predict(fit)
x=diamond$carat;y=diamond$price
n<-length(y)
plot(x,e)
yhat<-predict(fit)
x=diamond$carat;y=diamond$price
n<-length(y)
for(i in 1:n){
lines(c(x[i],x[i]),c(y[i],yhat[i]),col="red",lwd=2 )
}
e<-resid(fit)
plot(x,e)
plot(x,e)
abline(h=0)
for (i in 1:n){
lines(c(x[i],x[i]),c(0,e[i]),col="red")
}
x<-runif(100,-3,3)
y<-x+sin(x)+rnorm(100,0.1,0.1)
fit<-lm(y~x)
rm(g)
g<-ggplot(data=data.frame(x=x,y=y),aes(x,y))
g<-g+geom_point(size=3,col="black",alpha=0.2)
g<-g+geom_point(size=2,col="red",alpha=0.3)
g<-g+geom_smooth(method="lm")
g
e<-resid(fit)
plot(x,e)
abline(h=0)
#Heteroscedasticity is unequal variance which can be emphasized using residual plots.
#basically linear model predicts good at low x values but more variation at big x values.
x<-runif(100,0,0.6)
y<-x+rnorm(100,mean=0,sd=.001*x)
plot(x,y)
fit<-lm(y~x)
abline(fit)
e<-resid(fit)
plot(x,e)
plot(x,e)
abline(h=0)
plot(x,e)
