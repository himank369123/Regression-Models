cbind(r2,rmse)
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.1))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=10),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
knots
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.2))
knots
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.2))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=6),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=5),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=1),se=FALSE,col='red',size=2)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=1),se=FALSE,col='red',size=2)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=20),se=FALSE,col='red',size=2)
g
knots<-quantile(train.data$lstat,p=c(0.25,0.5,0.75))
knots
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-g+geom_point(size=2,col='blue')
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g
g<-g+geom_point(size=2,col='blue')
g<-g+geom_point(size=2,col='blue')
g
cbind(r2,rmse)
library(mgcv)
mgcvfit<-gam(medv~s(lstat),train.data)
mgcvfit<-gam(medv~s(lstat),data=train.data)
predictions<-predict(mgcvfit,test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
summary(mgcvfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="gam",formula = y~s(x),se=FALSE,col='red',size=2)
g
gam<-cbind(r2,rmse)
gam
spline<-cbind(r2,rmse)
spline<-cbind(r2,rmse)
spline
logtrans<-cbind(r2,rmse)
logtrans
sixorderpol<-cbind(r2,rmse)
sixorderpol
twoorderpol<-cbind(r2,rmse)
linear<-cbind(r2,rmse)
rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
df<-rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c('linear','twoorderpol','sixorderpol','logtrans','spline','gam')
df
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
linear<-cbind(r2,rmse)
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
twoorderpol<-cbind(r2,rmse)
#when data doesnt follow a complete linear trend we must resort to non linear regression. for E.G:
library(tidyverse)
library(caret)
data('Boston',package = "MASS")
head(Boston)
training.samples<-Boston$medv %>% createDataPartition(p=0.8,list=F)
train.data<-Boston[training.samples,]
test.data<-Boston[-training.samples,]
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth()
#clearly the data isnt linear.
#Linear model:
head(train.data)
lmfit<-lm(data=train.data,medv~lstat)
summary(lmfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(col='blue',size=2)
g<-g+geom_smooth(method="lm",se=F,color='red',lwd=2)
g
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
linear<-cbind(r2,rmse)
linear
#Non Linear regression:
# 1) Polynomial regression
# 2) log transformation
# 3) spline regression
#1) POLYNOMIAL REGRESSION:
pn2fit<-lm(data=train.data,medv~lstat+I(lstat^2))
#OR
pn2fit<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pn2fit)
#pvalues indicate both degrees significant
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
twoorderpol<-cbind(r2,rmse)
twoorderpol
##similarly for 6 order polynomial:
pn6fit<-lm(data=train.data,medv~poly(lstat,6,raw=FALSE))
summary(pn6fit)
#pvalues indicate only degrees upto 5 are significant i.e can discard 6th degree term.
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,6,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
sixorderpol<-cbind(r2,rmse)
sixorderpol
#2) Log transformation:
logfit<-lm(data=train.data,medv~log(lstat))
summary(logfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~log(x),se=FALSE,col='red',size=2)
g
predictions<-predict(logfit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
logtrans<-cbind(r2,rmse)
logtrans
#3) spline regression:
#use knots to model non linear trends.
library(splines)
knots<-quantile(train.data$lstat,p=c(0.25,0.5,0.75))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
spline<-cbind(r2,rmse)
spline
##mgcv provides generalized additive models to fit spline regression autommatically without specifying knots
library(mgcv)
mgcvfit<-gam(medv~s(lstat),data=train.data)
summary(mgcvfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="gam",formula = y~s(x),se=FALSE,col='red',size=2)
g
predictions<-predict(mgcvfit,test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
gam<-cbind(r2,rmse)
gam
df<-rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c('linear','twoorderpol','sixorderpol','logtrans','spline','gam')
df
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
##fitting linear model:
lmfit<-lm(data=gaData,visits~julian)
load("C:/Users/himank/Documents/R/COURSERA DS/7.regression models/gaData.rda")
##POISOON REGRSSION:
#### THe outcome follow poission dist with mean u
#### link function is log(u)
#### log(u)=b0+b1x
#### e(b0+b1x)=u where u is mean or geometric mean for outocomes.
data(gaData)
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
##fitting linear model:
lmfit<-lm(data=gaData,visits~julian)
summary(lmfit)
#suggests 0.028% increase in visits per day.
g<-g+geom_smooth(method="lm",color='red',se=F,size=2)
g
## now fitting poisson regression model:
glmfit<-glm(data=gaData,visits~julian,family="poisson")
summary(glmfit)
exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
##we can model proportions too.
##for example proportion of webhits originating from simply statistics compared to toal visits.
## then we want to model log(simplystats/visits)=b0+b1x
##it turns out to be normal model +offset . where offset is log of denominator or total count.
glmfit2<-glm(data=gaData,simplystats~julian,offset=log(visits+1),family="poisson")
summary(glmfit2)
exp(coef(glmfit2))
plot(gaData$julian,glmfit2$fitted,col="blue",pch=19)
points(gaData$julian,glmfit$fitted,col="red",pch=19)
##actual points of proportion vs model(line)
plot(gaData$julian,gaData$simplystats/(gaData$visits+1),col="grey")
lines(gaData$julian,glmfit2$fitted/(gaData$visits+1),col="blue")
folds$train[[1]]
library(caret)
library(kernlab)
library(RANN)
data(spam)
set.seed(123)
partition<-createDataPartition(y=spam$type,p=0.75,list=F)
training<-spam[partition,]
testing<-spam[-partition,]
dim(training)
args(trainControl)
model<-train(type~.,data=training,method='glm')
model$finalModel
predictions<-predict(model,newdata=testing)
confusionMatrix(predictions,testing$type)
##cross validataion using k folds
folds<-createFolds(y=spam$type,k=10,list=TRUE,returnTrain = T)
#folds returns indices of training set.
sapply(folds,length)
##cross validation using resampling
folds<-createResample(y=spam$type,list=TRUE,times=10)
#folds returns indices of training set.
folds[[1]]
#folds returns indices of training set.
folds[[2]]
#cross validation
library(caret)
data(iris)
partition<-createDataPartition(y=iris$Species,p=0.75,list=F)
train<-iris[partition,]
test<-iris[-partition,]
model1<-train(data=train,Species~.,method='rpart')
predict(model1,test)
confusionMatrix(predict(model1,test),test$Species)
#bootstrapping resampling
train_control<-trainControl(method='boot',number=100)
model2<-train(Species~.,data=iris,trControl=train_control,method='rpart')
model
model1
model1
model2
#kfold cross validation
train_control<-trainControl(method='cv',number=10)
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model<-train(Species~.,data=iris,trControl=train_control,method='rpart',)
# train the model
model<-train(Species~.,data=iris,trControl=train_control,method='rpart')
# train the model
model<-train(Species~.,data=iris,trControl=train_control,method='rpart')
model
model1<-train(data=train,Species~.,method='rpart')
confusionMatrix(predict(model1,test),test$Species)
model1
# train the model
model<-train(Species~.,data=iris,trControl=train_control,method='rpart',repeats=3)
model
model
#repeated kfold cross validation
train_control<-trainControl(method='repeatedcv',number=10,repeats=3)
model4<-train(Species~.,data=iris,trControl=train_control,method='rpart')
model4
#leave one out cross validation
train_control<-trainControl(method='LOOCV')
model5<-train(Species~.,data=iris,trControl=train_control,method='rpart')
model5
args(trainControl)
##cross validation using resampling
folds<-createResample(y=spam$type,list=TRUE,times=10)
sapply(folds,length)
source('~/R/COURSERA DS/8.machine learning/CrossValidation.R')
library(mgcv)
#Model Based Prediction:
library(caret)
partition<-createDataPartition(y=iris$Species,y=0.7,list=F)
partition<-createDataPartition(y=iris$Species,y=0.7,list=F)
partition<-createDataPartition(y=iris$Species,p=0.7,list=F)
training<-iris[partition]
training<-iris[partition,]
testing<-iris[-partition,]
model<-train(Species~.,data=training,method='lda')
modelb<-train(Species~.,data=training,method='nb')
plda<-predict(model,testing)
pnb<-predict(modelb,testing)
table(plda,pnb)
model2<-train(Species~.,data=training,method='qda')
pldb<-predict(model2,testing)
table(plda,pldb)
modelLDA<-train(Species~.,data=training,method='lda')
modelQDA<-train(Species~.,data=training,method='qda')
modelNB<-train(Species~.,data=training,method='nb')
modelLDA<-train(Species~.,data=training,method='lda')
modelQDA<-train(Species~.,data=training,method='qda')
modelNB<-train(Species~.,data=training,method='nb')
pQDA<-predict(model2,testing)
pLDA<-predict(model,testing)
pNB<-predict(modelb,testing)
table(plda,pldb)
table(pQDA,pLDA)
table(pQDA,pLDA,pNB)
library(caret)
library(AppliedPredictiveModeling)
data("segmentationOriginal")
partition<-createDataPartition(y=segmentationOriginal$Case,p=0.7,list=F)
training<-segmentationOriginal[partition]
training<-segmentationOriginal[partition,]
testing<-segmentationOriginal[-partition,]
set.seed(125)
segmentationOriginal
CART<-train(Case~.,training,method='rpart')
CART$finalModel
testing
newdf<-data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2,
TotalIntench2 = 50,000, FiberWidthCh1 = 10,VarIntenCh4 = 100,
TotalIntench2 = 57,000, FiberWidthCh1 = 8,VarIntenCh4 = 100,
FiberWidthCh1 = 8,VarIntenCh4 = 100, PerimStatusCh1=2)
newdf
newdf<-data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2;
TotalIntench2 = 50,000, FiberWidthCh1 = 10,VarIntenCh4 = 100;
TotalIntench2 = 57,000, FiberWidthCh1 = 8,VarIntenCh4 = 100;
FiberWidthCh1 = 8,VarIntenCh4 = 100, PerimStatusCh1=2)
newdf<-data.frame(x1=23000,50000,57000,x2= 10,10,8,x3= 2,100,100);
newdf
newdf<-data.frame(x1=c(23000,50000,57000),x2=c(10,10,8),x3= c(2,100,100));
newdf
names(newdf)<-
names(testing)
names(testing)
names(newdf)<-c("TotalIntench2","FiberWidthCh1","PerimStatusCh1")
newdf
predict(CART,newdf)
data("segmentationOriginal")
training<-segmentationOriginal[segmentationOriginal$Case =="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="TEST",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
training
testing
set.seed(125)
segmentationOriginal
CART<-train(Case~.,training,method='rpart')
CART$finalModel
testing$TotalIntenCh1
newdf<-data.frame(x1=c(23000,50000,57000),x2=c(10,10,8),x3= c(2,100,100));
names(newdf)<-c("TotalIntench2","FiberWidthCh1","PerimStatusCh1")
newdf
predict(CART,newdf)
library(ElemStatLearn)
library(rpart)
library(rattle)
fancyRpartPlot(CART$finalModel)
CART<-train(Case~.,training,method='rpart')
CART$finalModel
testing$TotalIntenCh1
newdf<-data.frame(x1=c(23000,50000,57000),x2=c(10,10,8),x3= c(2,100,100));
names(newdf)<-c("TotalIntench2","FiberWidthCh1","PerimStatusCh1")
library(ElemStatLearn)
library(rattle)
fancyRpartPlot(CART$finalModel)
data("segmentationOriginal")
training<-segmentationOriginal[segmentationOriginal$Case =="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
segmentationOriginal
CART<-train(Case~.,training,method='rpart')
CART$finalModel
CART<-train(Case~.,training,method='rpart')
CART<-train(Case~.,training,method='rpart')
training<-segmentationOriginal[segmentationOriginal$Case =="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
segmentationOriginal
CART<-train(Case~.,training,method='rpart')
training$Case
training$Case=="Train"
sum(training$Case=="Train")
nrow(training)
CART<-train(Case~.,training,method='rpart')
training$Case
CART<-train(Class~.,training,method='rpart')
CART$finalModel
testing$TotalIntenCh1
newdf<-data.frame(x1=c(23000,50000,57000),x2=c(10,10,8),x3= c(2,100,100));
names(newdf)<-c("TotalIntench2","FiberWidthCh1","PerimStatusCh1")
library(ElemStatLearn)
fancyRpartPlot(CART$finalModel)
library(rattle)
library(pgmm)
data(olive)
olive = olive[,-1]
library(pgmm)
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
model<-train(Area~.,olive,method='rpart')
newdata = as.data.frame(t(colMeans(olive)))
newdata
olive
model<-train(Area~.,olive,method='rpart')
predict(model,newdata)
testSA = SAheart[-train,]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model<-train(chd~.,trainSA,method="glm",family="binomial")
trainSA$chd
model<-train(as.factor(chd)~.,trainSA,method="glm",family="binomial")
model<-train(as.factor(chd)~.,trainSA,method="glm",family="binomial")
names(trainSA)
model<-train(as.factor(chd)~age+alchol+obesity+tobacco+typea+ldl,trainSA,method="glm",family="binomial")
model<-train(as.factor(chd)~age+alcohol+obesity+tobacco+typea+ldl,trainSA,method="glm",family="binomial")
model
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,predict(model,trainSA))
predict(model,trainSA
predict(model,trainSA)
predict(model,trainSA)
model<-train(chd~age+alcohol+obesity+tobacco+typea+ldl,trainSA,method="glm",family="binomial")
model <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,data=trainSA,method="glm",family="binomial")
model<-train(chd~age+alcohol+obesity+tobacco+typea+ldl,trainSA,method="glm",family="binomial")
names(trainSA)
model
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,predict(model,trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y<-factor(vowel.test$y)
vowel.train$y<-factor(vowel.train$y)
set.seed(33833)
model<-train(y~.,vowel.train,method='rf')
varImp(model)
?randomForest::randomForest
randomForest::randomForest(y~.,vowel.train)
model2<-randomForest::randomForest(y~.,vowel.train)
varImp(model2)
model
model2<-randomForest::randomForest(y~.,vowel.train)
model2<-randomForest(y~.,vowel.train)
library(randomForest)
vowel.test$y<-factor(vowel.test$y)
model2<-randomForest(y~.,vowel.train)
model2<-randomForest(y~.,vowel.train)
varImp(model2)
model2
model2$forest
model2$call
model2
model2$
model2$type
model2$predicted
model2$importance
model
model$modelInfo
