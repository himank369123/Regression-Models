exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
## we can see a sligt curve in case of glm
##we can model proportions too.
##for example proportion of webhits originating from simply statistics compared to toal visits.
## then we want to model log(simplystats/visits)=b0+b1x
##it turns out to be normal model +offset . where offset is log of denominator or total count.
glmfit2<-glm(data=gaData,simplystats~julian,offset=log(visits+1),family="poisson")
summary(glmfit2)
exp(coef(glmfit2))
plot(gaData$julian,glmfit2$fitted,col="blue",pch=19)
points(gaData$julian,glmfit$fitted,col="red",pch=19)
##glmfit vs glmfit2
##actual points of proportion vs model(line)
plot(gaData$julian,gaData$simplystats/(gaData$visits+1),col="grey")
lines(gaData$julian,glmfit2$fitted/(gaData$visits+1),col="blue")
load("C:/Users/himank/Documents/R/COURSERA DS/7.regression models/gaData.rda")
##POISOON REGRSSION:
#### THe outcome follow poission dist with mean u
#### link function is log(u)
#### log(u)=b0+b1x
#### e(b0+b1x)=u where u is mean or geometric mean for outocomes.
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
##fitting linear model:
lmfit<-lm(data=gaData,visits~julian)
summary(lmfit)
#suggests 0.028% increase in visits per day.
g<-g+geom_smooth(method="lm",color='red',se=F,size=2)
g
## now fitting poisson regression model:
glmfit<-glm(data=gaData,visits~julian,family="poisson")
summary(glmfit)
exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
## we can see a sligt curve in case of glm
##we can model proportions too.
##for example proportion of webhits originating from simply statistics compared to toal visits.
## then we want to model log(simplystats/visits)=b0+b1x
##it turns out to be normal model +offset . where offset is log of denominator or total count.
glmfit2<-glm(data=gaData,simplystats~julian,offset=log(visits+1),family="poisson")
summary(glmfit2)
exp(coef(glmfit2))
plot(gaData$julian,glmfit2$fitted,col="blue",pch=19)
points(gaData$julian,glmfit$fitted,col="red",pch=19)
##glmfit vs glmfit2
##actual points of proportion vs model(line)
plot(gaData$julian,gaData$simplystats/(gaData$visits+1),col="grey")
lines(gaData$julian,glmfit2$fitted/(gaData$visits+1),col="blue")
summary(lmfit)
exp(coef(glmfit)
exp(coef(glmfit))
exp(coef(glmfit))
exp(coef(glmfit2))
#when data doesnt follow a complete linear trend we must resort to non linear regression. for E.G:
library(tidyverse)
#when data doesnt follow a complete linear trend we must resort to non linear regression. for E.G:
install.packages("tidyverse")
#when data doesnt follow a complete linear trend we must resort to non linear regression. for E.G:
library(tidyverse)
library(carat)
install.package(carat)
install.packages("carat")
library(carat)
data('Boston',package = "MASS")
head(Boston)
training.samples<-Boston$medv %>% createDataPartition()
library(caret)
training.samples<-Boston$medv %>% createDataPartition(p=0.8,list=F)
training.samples
train.data<-Boston[training.samples,]
test.data<-Boston[-training.samples,]
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth()
#clearly the data isnt linear.
#Linear model:
head(train.data)
lmfit<-lm(data=train.data,medv~lstat)
summary(lmfit)
plot(train.data$lstat,train.data$medv)
abline(lmfit)
plot(train.data$lstat,train.data$medv,pch=19,col="blue")
abline(lmfit,col='red',lwd=2)
#1) POLYNOMIAL REGRESSION:
pnlm<-lm(data=train.data,medv~lstat+I(lstat^2))
summary(pnlm)
plot(train.data$lstat,train.data$medv,pch=19,col="blue")
abline(pnlm,col='red',lwd=2)
#Non Linear regression:
# 1) Polynomial regression
# 2) log transformation
# 3) spline regression
poly(lstat,5,raw=F)
#Non Linear regression:
# 1) Polynomial regression
# 2) log transformation
# 3) spline regression
poly(train.data$lstat,5,raw=F)
#OR
pnlm<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pnlm)
plot(train.data$lstat,train.data$medv,pch=19,col="blue")
abline(pnlm,col='red',lwd=2)
abline(pnlm,col='red',lwd=2)
plot(train.data$lstat,train.data$medv,pch=19,col="blue")
abline(pnlm,col='red',lwd=2)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(aes(color='blue'))
g
g<-g+geom_point(aes(col='blue'))
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(aes(col='blue'))
g
g<-g+geom_point(col='blue')
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(col='blue')
g
g<-g+geom_point(col='blue',size=2)
g
g<-g+geom_smooth(method="lm")
g
g<-g+geom_smooth(method="lm",se=F)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(col='blue',size=2)
g<-g+geom_smooth(method="lm",se=F)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(col='blue',size=2)
g<-g+geom_smooth(method="lm",se=F,color='red',lwd=2)
g
#OR
pnlm<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pnlm)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE))
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE))
g
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red')
g
summary(pnlm)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=2)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=1)
g
#OR
pn2fit<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pn2fit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=2)
g
summary(pn2fit)
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
#Linear model performance:
predictions<-predict(fit,newdata=test.data)
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
cbind(r2,rmse)
summary(pn2fit)
##similarly for 6 order polynomial:
pn6fit<-lm(data=train.data,medv~poly(lstat,6,raw=FALSE))
summary(pn6fit)
#pvalues indicate only degrees upto 5 are significant i.e can discard 6th degree term.
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,6,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
##similarly for 6 order polynomial:
pn6fit<-lm(data=train.data,medv~poly(lstat,6,raw=FALSE))
summary(pn6fit)
#pvalues indicate only degrees upto 5 are significant i.e can discard 6th degree term.
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,6,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
#Non Linear regression:
# 1) Polynomial regression
# 2) log transformation
# 3) spline regression
#1) POLYNOMIAL REGRESSION:
pn2fit<-lm(data=train.data,medv~lstat+I(lstat^2))
#OR
pn2fit<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pn2fit)
#pvalues indicate both degrees significant
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
##similarly for 6 order polynomial:
pn6fit<-lm(data=train.data,medv~poly(lstat,6,raw=FALSE))
summary(pn6fit)
#pvalues indicate only degrees upto 5 are significant i.e can discard 6th degree term.
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_smooth(method="lm",formula = y~poly(x,6,raw=FALSE),se=FALSE,col='red',size=2)
g<-g+geom_point(size=2,col='blue')
g
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
#2) Log transformation:
logfit<-lm(data=train.data,medv~log(lstat))
summary(logfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~log(x),se=FALSE,col='red',size=2)
g
predictions<-predict(logfit),newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
predictions<-predict(logfit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
predictions<-predict(logfit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
#3) spline regression:
#use knots to model non linear trends.
library(splines)
knots<-quantiles(train.data$lstat,p=c(0.25,0.5,0.75))
knots<-quantile(train.data$lstat,p=c(0.25,0.5,0.75))
splinefit<-lm(medv~bs(lstat,knots=knots))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.1))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=10),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
knots
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.2))
knots
knots<-quantile(train.data$lstat,p=seq(0,1,by=0.2))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=6),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=5),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=1),se=FALSE,col='red',size=2)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=1),se=FALSE,col='red',size=2)
g
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=20),se=FALSE,col='red',size=2)
g
knots<-quantile(train.data$lstat,p=c(0.25,0.5,0.75))
knots
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-g+geom_point(size=2,col='blue')
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g
g<-g+geom_point(size=2,col='blue')
g<-g+geom_point(size=2,col='blue')
g
cbind(r2,rmse)
library(mgcv)
mgcvfit<-gam(medv~s(lstat),train.data)
mgcvfit<-gam(medv~s(lstat),data=train.data)
predictions<-predict(mgcvfit,test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
cbind(r2,rmse)
summary(mgcvfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="gam",formula = y~s(x),se=FALSE,col='red',size=2)
g
gam<-cbind(r2,rmse)
gam
spline<-cbind(r2,rmse)
spline<-cbind(r2,rmse)
spline
logtrans<-cbind(r2,rmse)
logtrans
sixorderpol<-cbind(r2,rmse)
sixorderpol
twoorderpol<-cbind(r2,rmse)
linear<-cbind(r2,rmse)
rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
df<-rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c('linear','twoorderpol','sixorderpol','logtrans','spline','gam')
df
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
linear<-cbind(r2,rmse)
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
twoorderpol<-cbind(r2,rmse)
#when data doesnt follow a complete linear trend we must resort to non linear regression. for E.G:
library(tidyverse)
library(caret)
data('Boston',package = "MASS")
head(Boston)
training.samples<-Boston$medv %>% createDataPartition(p=0.8,list=F)
train.data<-Boston[training.samples,]
test.data<-Boston[-training.samples,]
ggplot(train.data, aes(lstat, medv) ) +
geom_point() +
stat_smooth()
#clearly the data isnt linear.
#Linear model:
head(train.data)
lmfit<-lm(data=train.data,medv~lstat)
summary(lmfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(col='blue',size=2)
g<-g+geom_smooth(method="lm",se=F,color='red',lwd=2)
g
#Linear model performance:
predictions<-predict(lmfit,newdata=test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
linear<-cbind(r2,rmse)
linear
#Non Linear regression:
# 1) Polynomial regression
# 2) log transformation
# 3) spline regression
#1) POLYNOMIAL REGRESSION:
pn2fit<-lm(data=train.data,medv~lstat+I(lstat^2))
#OR
pn2fit<-lm(data=train.data,medv~poly(lstat,2,raw=FALSE))
summary(pn2fit)
#pvalues indicate both degrees significant
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,2,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn2fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
twoorderpol<-cbind(r2,rmse)
twoorderpol
##similarly for 6 order polynomial:
pn6fit<-lm(data=train.data,medv~poly(lstat,6,raw=FALSE))
summary(pn6fit)
#pvalues indicate only degrees upto 5 are significant i.e can discard 6th degree term.
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~poly(x,6,raw=FALSE),se=FALSE,col='red',size=2)
g
predictions<-predict(pn6fit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
sixorderpol<-cbind(r2,rmse)
sixorderpol
#2) Log transformation:
logfit<-lm(data=train.data,medv~log(lstat))
summary(logfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~log(x),se=FALSE,col='red',size=2)
g
predictions<-predict(logfit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
logtrans<-cbind(r2,rmse)
logtrans
#3) spline regression:
#use knots to model non linear trends.
library(splines)
knots<-quantile(train.data$lstat,p=c(0.25,0.5,0.75))
splinefit<-lm(medv~bs(lstat,knots=knots),data=train.data)
summary(splinefit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="lm",formula = y~bs(x,df=3),se=FALSE,col='red',size=2)
g
predictions<-predict(splinefit,newdata = test.data)
##model performance:
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
spline<-cbind(r2,rmse)
spline
##mgcv provides generalized additive models to fit spline regression autommatically without specifying knots
library(mgcv)
mgcvfit<-gam(medv~s(lstat),data=train.data)
summary(mgcvfit)
g<-ggplot(data=train.data,aes(lstat,medv))
g<-g+geom_point(size=2,col='blue')
g<-g+geom_smooth(method="gam",formula = y~s(x),se=FALSE,col='red',size=2)
g
predictions<-predict(mgcvfit,test.data)
r2<-R2(predictions,test.data$medv)
rmse<-RMSE(predictions,test.data$medv)
gam<-cbind(r2,rmse)
gam
df<-rbind(linear,twoorderpol,sixorderpol,logtrans,spline,gam)
rownames(df)<-c('linear','twoorderpol','sixorderpol','logtrans','spline','gam')
df
